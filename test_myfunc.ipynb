{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Mysoftmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        result = input - input.max()\n",
    "        result = result.exp()\n",
    "        result = result/result.sum()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # grad_outputにはforwardのinputに入っている変数の値が入っている。\n",
    "        result,  = ctx.saved_tensors\n",
    "        J = torch.zeros(result.size()[-1], result.size()[-1])\n",
    "        print(result)\n",
    "        for i in range(result.size()[-1]):\n",
    "            for j in range(result.size()[-1]):\n",
    "                if i==j:\n",
    "                    J[i][j] = result[0][i]*(1-result[0][i])\n",
    "                else:\n",
    "                    J[i][j] = -result[0][i]*result[0][j]\n",
    "        grad_output = torch.mm(grad_output, J)\n",
    "        return grad_output\n",
    "    \n",
    "# このailiasをやらないとtensorが出力にならない\n",
    "softmax = Mysoftmax.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1.0,2.0,3.0]], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652]], grad_fn=<MysoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "b = softmax(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652]], grad_fn=<MysoftmaxBackward>)\n",
      "tensor([[-0.1418, -0.1408,  0.2826]])\n"
     ]
    }
   ],
   "source": [
    "b.backward(a)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1418, -0.1408,  0.2826])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], dtype=torch.float32, requires_grad=True)\n",
    "y = x - x.max()\n",
    "y = y.exp()\n",
    "result = y/y.sum()\n",
    "result.backward(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ueno/.pyenv/versions/3.6.1/envs/ntm/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "c_a = torch.tensor([[1.0,2.0,3.0]], dtype=torch.float32, requires_grad=True)\n",
    "c = F.softmax(c_a)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1418, -0.1408,  0.2826]])\n"
     ]
    }
   ],
   "source": [
    "c.backward(c_a)\n",
    "print(c_a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "c_a = torch.tensor([[1.0,2.0,3.0]], dtype=torch.float32, requires_grad=True)\n",
    "c = F.softmax(c_a, dim = 0)\n",
    "print(c)\n",
    "c.backward(c_a)\n",
    "print(c_a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.1418, -0.1408,  0.2826]])\n"
     ]
    }
   ],
   "source": [
    "c_a = torch.tensor([[1.0,2.0,3.0]], dtype=torch.float32, requires_grad=True)\n",
    "c = F.softmax(c_a, dim=1)\n",
    "print(c)\n",
    "c.backward(c_a)\n",
    "print(c_a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[-0.1418, -0.1408,  0.2826]])\n"
     ]
    }
   ],
   "source": [
    "c_a = torch.tensor([[1.0,2.0,3.0]], dtype=torch.float32, requires_grad=True)\n",
    "c = F.softmax(c_a, dim=1)\n",
    "print(c)\n",
    "c.backward(c_a)\n",
    "print(c_a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# テンソルを作成\n",
    "# requires_grad=Falseだと微分の対象にならず勾配はNoneが返る\n",
    "x = torch.tensor([0.0, 1.0, 4.0], requires_grad=True)\n",
    "\n",
    "# 計算グラフを構築\n",
    "# y = 2 * x + 3\n",
    "y = F.relu(x)\n",
    "\n",
    "# 勾配を計算\n",
    "y.backward(x)\n",
    "\n",
    "# 勾配を表示\n",
    "print(x.grad)  # dy/dx = w = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch tutorialから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 9., 16.],\n",
      "        [25., 36.]], grad_fn=<MulBackward0>) tensor(21.5000, grad_fn=<MeanBackward1>)\n",
      "tensor([[1.5000, 2.0000],\n",
      "        [2.5000, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# x = torch.ones(2, 2, requires_grad=True)\n",
    "x = torch.tensor([[1,2],[3,4]], requires_grad=True, dtype=torch.float32)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3 # これはアダマール積(要素積，　行列の掛け算ではない)\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmaxのヤコビアンの計算\n",
    "\n",
    "単純な実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0,2.0,3.0], dtype=torch.float32)\n",
    "\n",
    "# J = [a[i]*(1-a[i]) if i==j else -a[i]*a[j] for i in range(len(a)) for j in range(len(a))]\n",
    "J = [a[i]*(1-a[i]) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.), tensor(-2.), tensor(-6.)]\n"
     ]
    }
   ],
   "source": [
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0., -2., -3.]],\n",
      "\n",
      "        [[-2., -2., -6.]],\n",
      "\n",
      "        [[-3., -6., -6.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0,2.0,3.0], dtype=torch.float32)\n",
    "J = []\n",
    "for i in range(len(a)):\n",
    "    tmp = []\n",
    "    for j in range(len(a)):\n",
    "        if i==j:\n",
    "            tmp.append(a[i]*(1-a[i]))\n",
    "        else:\n",
    "            tmp.append(-a[i]*a[j])\n",
    "    J.append([tmp])\n",
    "J = torch.tensor(J)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=True)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2957,  0.7737, -0.4852,  2.5937],\n",
      "        [ 0.3693,  1.4469,  0.0763,  0.6498],\n",
      "        [-1.5699,  1.8273,  0.4325, -2.0887],\n",
      "        [-0.3461, -2.0550, -0.7831, -0.2714]])\n",
      "tensor([ 2.5865,  2.5423, -1.3988, -3.4555])\n",
      "tensor([-1.8424,  1.9928, -0.7594,  0.8835])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "print(a)\n",
    "print(torch.sum(a, 1))\n",
    "print(torch.sum(a, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0,2.0,3.0], dtype=torch.float32)\n",
    "\n",
    "print(a[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Mysoftmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        y = x - max(x)\n",
    "        y = y.exp()\n",
    "        result = y/y.sum()\n",
    "        ctx.save_for_backward(x, result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, result = ctx.saved_tensors\n",
    "        #print(result,x)\n",
    "        #\"\"\"\n",
    "        J = []\n",
    "        for i in range(len(result)):\n",
    "            tmp = []\n",
    "            for j in range(len(result)):\n",
    "                if i==j:\n",
    "                    tmp.append(result[i].item()*(1-result[i].item()))\n",
    "                else:\n",
    "                    tmp.append(-result[i].item()*result[j].item())\n",
    "            #J.append(torch.tensor(tmp))\n",
    "            J.append(tmp)\n",
    "        J = torch.tensor(J)\n",
    "        #print(\"J\\n\",J)\n",
    "        grad_input = torch.mv(J, x)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Mycossim(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, key_vec):\n",
    "        #k_1 = k.view(-1)\n",
    "        #x = x.view(x.size()[1], x.size()[2])\n",
    "        #y = [torch.dot(x/x.norm(), k/k.norm()) for x in x]\n",
    "        #y = torch.tensor(y)\n",
    "        \"\"\"\n",
    "        z = []\n",
    "        for i in x:\n",
    "            z.append(torch.dot(i/ torch.norm(i, dim=-1), k / torch.norm(k)))\n",
    "        \"\"\"\n",
    "        print(\"input\", input)\n",
    "        print(\"key\", key_vec)\n",
    "        #mem_k = (mem*key_vec).sum()\n",
    "        input_norm = input.norm()\n",
    "        key_vec_norm = key_vec.norm()\n",
    "        #result = mem_k/(max(mem_norm*key_vec_norm, 1e-8))\n",
    "        result = (input*key_vec).sum()/(torch.max(input_norm*key_vec_norm, 1e-8))\n",
    "        ctx.save_for_backward(input, key_vec, result)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        mem, key_vec, result = ctx.saved_tensors\n",
    "        print(\"grad_output\", grad_output)\n",
    "        \"\"\"print(m)\n",
    "        print(torch.norm(m, dim=-1))\n",
    "        print(m.norm())\n",
    "        print(torch.norm(m, dim=-1)**2)\n",
    "        print(m.pow(2).sum())\n",
    "        \"\"\"\n",
    "        #print(result*k/k.pow(2).sum())\n",
    "        eps = torch.tensor([[1e-8]])\n",
    "        m = mem\n",
    "        k = key_vec\n",
    "        m_norm = torch.norm(m)\n",
    "        k_norm = torch.norm(k)\n",
    "        m_grad = k/max(m_norm*k_norm, eps) - result*m/max(m_norm.pow(2), eps)\n",
    "        k_grad = m/max(m_norm*k_norm, eps) - result*k/max(k_norm.pow(2), eps)\n",
    "        print(m_grad)\n",
    "        print(k_grad)\n",
    "        #m_norm = torch.abs(m)\n",
    "        #k_norm = torch.abs(k)\n",
    "        #m_grad = k/torch.sum(m_norm*k_norm) - result*m/torch.sum(m_norm.pow(2))\n",
    "        #k_grad = m/torch.sum(m_norm*k_norm) - result*k/torch.sum(k_norm.pow(2))\n",
    "        m_grad = m_grad*m\n",
    "        k_grad = k_grad*k\n",
    "        return m_grad, k_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My cosine\n",
      "input tensor([[1., 2., 3.]], requires_grad=True)\n",
      "key tensor([[-1., -2., -3.]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (Tensor, float), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, Tensor out)\n * (Tensor input, int dim, bool keepdim, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-da795673cc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# このailiasをやらないとtensorが出力にならない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMycossim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-08da9a59efd6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, key_vec)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mkey_vec_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#result = mem_k/(max(mem_norm*key_vec_norm, 1e-8))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_norm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey_vec_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (Tensor, float), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, Tensor out)\n * (Tensor input, int dim, bool keepdim, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1.0, 2.0, 3.0]], requires_grad=True)\n",
    "b = torch.tensor([[-1.0,-2.0,-3.0]], requires_grad=True)\n",
    "#b = torch.tensor([[-3.0,-6.0,-2.0]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "print(\"\\nMy cosine\")\n",
    "# このailiasをやらないとtensorが出力にならない\n",
    "cosine_similarity = Mycossim.apply\n",
    "c = cosine_similarity(a,b)\n",
    "print(c)\n",
    "\n",
    "c.backward(a)\n",
    "\n",
    "print(\"a grad:\", a.grad)\n",
    "print(\"b grad:\", b.grad)\n",
    "\n",
    "\n",
    "# ------------------\n",
    "\n",
    "\n",
    "f_c = F.cosine_similarity(a,b, dim=-1)\n",
    "print(f_c)\n",
    "# aについて\n",
    "f_c.backward(a)\n",
    "print(\"a grad:\", a.grad)\n",
    "print(\"b grad:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0000, grad_fn=<DivBackward0>)\n",
      "a grad: tensor([2.9802e-08, 5.9605e-08, 0.0000e+00])\n",
      "b grad: tensor([-2.9802e-08, -5.9605e-08,  0.0000e+00])\n",
      "tensor(-1.0000, grad_fn=<MycossimBackward>)\n",
      "grad_output tensor(-6., grad_fn=<SumBackward0>)\n",
      "tensor([-7.4506e-09, -1.4901e-08, -1.4901e-08])\n",
      "tensor([7.4506e-09, 1.4901e-08, 1.4901e-08])\n",
      "a grad: tensor([ 2.2352e-08,  2.9802e-08, -4.4703e-08])\n",
      "b grad: tensor([-3.7253e-08, -8.9407e-08, -4.4703e-08])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "b = torch.tensor([-1.0,-2.0,-3.0], requires_grad=True)\n",
    "\n",
    "f_c = F.cosine_similarity(a,b, dim=-1)\n",
    "print(f_c)\n",
    "# aについて\n",
    "f_c.backward(b)\n",
    "print(\"a grad:\", a.grad)\n",
    "print(\"b grad:\", b.grad)\n",
    "\n",
    "# ----------\n",
    "# このailiasをやらないとtensorが出力にならない\n",
    "cosine_similarity = Mycossim.apply\n",
    "c = cosine_similarity(a,b)\n",
    "print(c)\n",
    "\n",
    "c.backward(b)\n",
    "\n",
    "print(\"a grad:\", a.grad)\n",
    "print(\"b grad:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -3., -12.,  -6.], grad_fn=<MulBackward0>)\n",
      "tensor(-21., grad_fn=<DotBackward>)\n",
      "tensor([-0.3333, -0.3333, -1.5000], grad_fn=<DivBackward0>)\n",
      "tensor([-0.3333, -0.3333, -1.5000], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a*b)\n",
    "print(torch.dot(a, b))\n",
    "print(a/b)\n",
    "print(torch.div(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.grad tensor([7.4506e-09, 1.4901e-08, 1.4901e-08])\n",
      "k.grad tensor([7.4506e-09, 1.4901e-08, 1.4901e-08])\n",
      "m_norm.grad None\n",
      "k_norm.grad None\n",
      "result None\n",
      "m_grad tensor([7.4506e-09, 1.4901e-08, 1.4901e-08])\n",
      "k_grad tensor([7.4506e-09, 1.4901e-08, 1.4901e-08])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "m = torch.tensor([1.0,2.0,3.0], dtype=torch.float32, requires_grad=True)\n",
    "k = torch.tensor([1.0,2.0,3.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "m_k = torch.sum(m*k)\n",
    "m_norm = torch.norm(m)\n",
    "k_norm = torch.norm(k)\n",
    "result = m_k/(max(m_norm*k_norm, 1e-8))\n",
    "\n",
    "result.backward(result)\n",
    "print(\"m.grad\", m.grad)\n",
    "print(\"k.grad\", k.grad)\n",
    "\n",
    "print(\"m_norm.grad\", m_norm.grad)\n",
    "print(\"k_norm.grad\", k_norm.grad)\n",
    "\n",
    "print(\"result\", result.grad)\n",
    "\n",
    "\n",
    "m = torch.tensor([1.0,2.0,3.0], dtype=torch.float32, requires_grad=True)\n",
    "k = torch.tensor([1.0,2.0,3.0], dtype=torch.float32, requires_grad=True)\n",
    "c = F.cosine_similarity(m,k, dim=-1)\n",
    "c.backward(c)\n",
    "print(\"m_grad\", m.grad)\n",
    "print(\"k_grad\", k.grad)\n",
    "\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.tensor([[[1.0,2.0,3.0],[4.0,5.0,6.0]]], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  4.,  9.],\n",
      "         [ 4., 10., 18.]]], grad_fn=<MulBackward0>)\n",
      "tensor([[14., 32.]], grad_fn=<SumBackward2>)\n",
      "tensor([[3.7417, 8.7750]], grad_fn=<NormBackward1>)\n",
      "tensor(3.7417, grad_fn=<NormBackward1>)\n",
      "tensor([[1.0000, 0.9746]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0000, 0.9746]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[[1.0,2.0,3.0],[4.0,5.0,6.0]]], dtype=torch.float32, requires_grad=True)\n",
    "k = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "eps = torch.tensor([[1e-8]])\n",
    "\n",
    "print(m*k)\n",
    "print((m*k).sum(dim=-1))\n",
    "print(m.norm(dim=-1))\n",
    "print(k.norm(dim=-1))\n",
    "print((m*k).sum(dim=-1)/torch.max(m.norm(dim=-1)*k.norm(dim=-1), eps))\n",
    "\n",
    "print(F.cosine_similarity(m, k, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2888, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
